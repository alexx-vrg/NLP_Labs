{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3 - Hello World Transformers ü§ó\n",
        "\n",
        "This notebook explores the Hugging Face Transformers library through various NLP tasks using pre-trained models.\n",
        "\n",
        "## Quick Overview of Transformer Applications\n",
        "\n",
        "Sample text for testing various pipeline tasks:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers pandas torch sentencepiece sacremoses -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
        "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
        "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
        "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
        "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
        "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
        "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Classification\n",
        "\n",
        "### Question 1: Understanding Pipelines\n",
        "\n",
        "**1. What is a `pipeline` in Hugging Face Transformers?**\n",
        "\n",
        "A `pipeline` is a high-level API that abstracts the complexity of using pre-trained models. It handles tokenization, model inference, and post-processing in a single callable object.\n",
        "\n",
        "**2. Three other available tasks:**\n",
        "- `question-answering`\n",
        "- `summarization`\n",
        "- `named-entity-recognition (ner)`\n",
        "\n",
        "**3. Default model behavior:**\n",
        "\n",
        "When no model is specified, the pipeline uses a default model for that task. To specify a model: `pipeline(\"task\", model=\"model-name\")`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>0.901547</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label     score\n",
              "0  NEGATIVE  0.901547"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outputs = classifier(text)\n",
        "pd.DataFrame(outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2: Text Classification Deep Dive\n",
        "\n",
        "**1. Default model:** `distilbert-base-uncased-finetuned-sst-2-english`\n",
        "\n",
        "**2. Training dataset:** SST-2 (Stanford Sentiment Treebank) - movie reviews for sentiment analysis.\n",
        "\n",
        "**3. Score meaning:** The score represents the model's confidence (probability) for the predicted label, ranging from 0 to 1.\n",
        "\n",
        "**4. Emotion classification model:** `bhadresh-savani/distilbert-base-uncased-emotion` classifies into 6 emotions (sadness, joy, love, anger, fear, surprise).\n",
        "\n",
        "---\n",
        "\n",
        "## Named Entity Recognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>entity_group</th>\n",
              "      <th>score</th>\n",
              "      <th>word</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ORG</td>\n",
              "      <td>0.879011</td>\n",
              "      <td>Amazon</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.990859</td>\n",
              "      <td>Optimus Prime</td>\n",
              "      <td>36</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LOC</td>\n",
              "      <td>0.999755</td>\n",
              "      <td>Germany</td>\n",
              "      <td>90</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.556570</td>\n",
              "      <td>Mega</td>\n",
              "      <td>208</td>\n",
              "      <td>212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PER</td>\n",
              "      <td>0.590255</td>\n",
              "      <td>##tron</td>\n",
              "      <td>212</td>\n",
              "      <td>216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ORG</td>\n",
              "      <td>0.669692</td>\n",
              "      <td>Decept</td>\n",
              "      <td>253</td>\n",
              "      <td>259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.498350</td>\n",
              "      <td>##icons</td>\n",
              "      <td>259</td>\n",
              "      <td>264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.775362</td>\n",
              "      <td>Megatron</td>\n",
              "      <td>350</td>\n",
              "      <td>358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.987854</td>\n",
              "      <td>Optimus Prime</td>\n",
              "      <td>367</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>PER</td>\n",
              "      <td>0.812096</td>\n",
              "      <td>Bumblebee</td>\n",
              "      <td>502</td>\n",
              "      <td>511</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  entity_group     score           word  start  end\n",
              "0          ORG  0.879011         Amazon      5   11\n",
              "1         MISC  0.990859  Optimus Prime     36   49\n",
              "2          LOC  0.999755        Germany     90   97\n",
              "3         MISC  0.556570           Mega    208  212\n",
              "4          PER  0.590255         ##tron    212  216\n",
              "5          ORG  0.669692         Decept    253  259\n",
              "6         MISC  0.498350        ##icons    259  264\n",
              "7         MISC  0.775362       Megatron    350  358\n",
              "8         MISC  0.987854  Optimus Prime    367  380\n",
              "9          PER  0.812096      Bumblebee    502  511"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ner_tagger = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
        "outputs = ner_tagger(text)\n",
        "pd.DataFrame(outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3: Named Entity Recognition\n",
        "\n",
        "**1. `aggregation_strategy=\"simple\"`:** Merges tokens belonging to the same entity, computing a simple average of their scores.\n",
        "\n",
        "**2. Entity types:**\n",
        "- **ORG:** Organization\n",
        "- **PER:** Person\n",
        "- **LOC:** Location\n",
        "- **MISC:** Miscellaneous\n",
        "\n",
        "**3. `##` prefix:** Indicates subword tokens from WordPiece tokenization. Words not in vocabulary are split into subwords.\n",
        "\n",
        "**4. Splitting issue:** \"Megatron\" and \"Decepticons\" are fictional names not in the training data (CoNLL-2003, which uses news articles), so the model struggles with them.\n",
        "\n",
        "**5. CoNLL-2003:** A benchmark dataset for NER containing news articles annotated with person, organization, location, and miscellaneous entities.\n",
        "\n",
        "---\n",
        "\n",
        "## Question Answering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.631292</td>\n",
              "      <td>335</td>\n",
              "      <td>358</td>\n",
              "      <td>an exchange of Megatron</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      score  start  end                   answer\n",
              "0  0.631292    335  358  an exchange of Megatron"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reader = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "question = \"What does the customer want?\"\n",
        "outputs = reader(question=question, context=text)\n",
        "pd.DataFrame([outputs])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4: Question Answering Systems\n",
        "\n",
        "**1. Type:** Extractive QA - the model extracts a span from the context rather than generating new text.\n",
        "\n",
        "**2. `start` and `end` indices:** Character positions in the original text where the answer is located.\n",
        "\n",
        "**3. SQuAD:** Stanford Question Answering Dataset - 100k+ reading comprehension questions based on Wikipedia articles.\n",
        "\n",
        "**4. Unanswerable questions:** Questions requiring reasoning beyond the text or world knowledge not in the context.\n",
        "\n",
        "**5. Generative QA example:** `google/flan-t5-base` can generate answers rather than extract spans.\n",
        "\n",
        "---\n",
        "\n",
        "## Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead. As a lifelong enemy of the Decepticons, I\n"
          ]
        }
      ],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "outputs = summarizer(text, max_length=56, clean_up_tokenization_spaces=True)\n",
        "print(outputs[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 5: Text Summarization\n",
        "\n",
        "**1. Extractive vs Abstractive:**\n",
        "- **Extractive:** Selects and concatenates important sentences from the original text\n",
        "- **Abstractive:** Generates new text that captures the main ideas\n",
        "\n",
        "**2. Default model:** `sshleifer/distilbart-cnn-12-6`\n",
        "- Abstractive model using DistilBART architecture\n",
        "- Trained on CNN/DailyMail news summarization dataset\n",
        "\n",
        "**3. `max_length` and `min_length`:** Control the output summary length in tokens. If `min_length > max_length`, it raises an error.\n",
        "\n",
        "**4. `clean_up_tokenization_spaces`:** Removes extra spaces around punctuation for cleaner output.\n",
        "\n",
        "**5. Two summarization models:**\n",
        "- Short texts: `facebook/bart-large-cnn`\n",
        "- Long documents: `google/long-t5-tglobal-base` (handles up to 16k tokens)\n",
        "\n",
        "---\n",
        "\n",
        "## Translation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket √∂ffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, Sie k√∂nnen mein Dilemma verstehen. Um das Problem zu l√∂sen, Ich fordere einen Austausch von Megatron f√ºr die Optimus Prime Figur habe ich bestellt. Eingeschlossen sind Kopien meiner Aufzeichnungen √ºber diesen Kauf. Ich erwarte, von Ihnen bald zu h√∂ren. Aufrichtig, Bumblebee.\n"
          ]
        }
      ],
      "source": [
        "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
        "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
        "print(outputs[0]['translation_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 6: Machine Translation\n",
        "\n",
        "**1. Helsinki-NLP/opus-mt-en-de architecture:**\n",
        "- MarianMT (based on Marian NMT framework)\n",
        "- OPUS = Open Parallel Corpus\n",
        "- MT = Machine Translation\n",
        "\n",
        "**2. English to French models:**\n",
        "- `Helsinki-NLP/opus-mt-en-fr`\n",
        "- `facebook/mbart-large-50-many-to-many-mmt`\n",
        "\n",
        "**3. Bilingual vs Multilingual:**\n",
        "- **Bilingual:** Trained on one language pair, typically higher quality for that pair\n",
        "- **Multilingual:** Handles multiple languages, more versatile but may sacrifice some quality\n",
        "\n",
        "---\n",
        "\n",
        "## Text Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n",
            "\n",
            "Customer Service Response:\n",
            "Dear Bumblebee, I am sorry to hear that your order was mixed up. The order status was not correct. Upon further review, it is clear that my order received defective parts. However, the order number was correct on the page and I was able to see the parts in stock. As I said, there is no exchange and it appears that the original order does not appear to have been filled. Therefore, you may be able (or please) contact me at a later time to clarify the status. Thank you again for your patience. My apologies for any inconvenience. Your service is on hold until further notice.\n"
          ]
        }
      ],
      "source": [
        "from transformers import set_seed\n",
        "set_seed(42)\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\", pad_token_id=50256)\n",
        "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
        "prompt = text + \"\\n\\nCustomer Service Response:\\n\" + response\n",
        "outputs = generator(prompt, max_new_tokens=150, do_sample=True, top_k=50, top_p=0.95, no_repeat_ngram_size=2, truncation=True)\n",
        "print(outputs[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 7: Text Generation\n",
        "\n",
        "**1. `set_seed(42)`:** Ensures reproducibility by fixing the random number generator state. Without it, each run produces different outputs due to sampling randomness.\n",
        "\n",
        "**2. Generation parameters:**\n",
        "- **temperature:** Controls randomness (lower = more deterministic, higher = more creative)\n",
        "- **top_k:** Limits sampling to top k most probable tokens\n",
        "- **do_sample:** Enables sampling vs greedy decoding\n",
        "\n",
        "**3. `pad_token_id = eos_token_id`:** GPT-2 doesn't have a dedicated pad token, so we use the end-of-sequence token for padding to avoid warnings.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
