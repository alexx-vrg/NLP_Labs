{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 4 - Training Transformer Models for Text Classification\n",
        "\n",
        "This notebook covers the complete pipeline for training a transformer model on an emotion classification task: dataset loading, tokenization, feature extraction, and model training.\n",
        "\n",
        "## 1. Dataset Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 16000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "emotions = load_dataset(\"emotion\")\n",
        "emotions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 16000\n",
            "First sample: {'text': 'i didnt feel humiliated', 'label': 0}\n",
            "Columns: ['text', 'label']\n",
            "Features: {'text': Value('string'), 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'])}\n"
          ]
        }
      ],
      "source": [
        "train_ds = emotions[\"train\"]\n",
        "print(f\"Training samples: {len(train_ds)}\")\n",
        "print(f\"First sample: {train_ds[0]}\")\n",
        "print(f\"Columns: {train_ds.column_names}\")\n",
        "print(f\"Features: {train_ds.features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>label_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i didnt feel humiliated</td>\n",
              "      <td>0</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i can go from feeling so hopeless to so damned...</td>\n",
              "      <td>0</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
              "      <td>3</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
              "      <td>2</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i am feeling grouchy</td>\n",
              "      <td>3</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label label_name\n",
              "0                            i didnt feel humiliated      0    sadness\n",
              "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
              "2   im grabbing a minute to post i feel greedy wrong      3      anger\n",
              "3  i am ever feeling nostalgic about the fireplac...      2       love\n",
              "4                               i am feeling grouchy      3      anger"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "emotions.set_format(type=\"pandas\")\n",
        "df = emotions[\"train\"][:]\n",
        "\n",
        "def label_int2str(row):\n",
        "    return emotions[\"train\"].features[\"label\"].int2str(row)\n",
        "\n",
        "df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
        "plt.title(\"Frequency of Classes\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\n",
        "df.boxplot(\"Words Per Tweet\", by=\"label_name\", grid=False, showfliers=False, color=\"black\")\n",
        "plt.suptitle(\"\")\n",
        "plt.xlabel(\"\")\n",
        "plt.show()\n",
        "\n",
        "emotions.reset_format()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tokenization\n",
        "\n",
        "Using DistilBERT tokenizer to convert text to token IDs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded: {'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "Tokens: ['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '.', '[SEP]']\n",
            "Vocab size: 30522\n",
            "Max length: 512\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "text = \"Tokenizing text is a core task of NLP.\"\n",
        "encoded_text = tokenizer(text)\n",
        "print(f\"Encoded: {encoded_text}\")\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"Max length: {tokenizer.model_max_length}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 2000/2000 [00:00<00:00, 14064.44 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['text', 'label', 'input_ids', 'attention_mask']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
        "print(emotions_encoded[\"train\"].column_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Extraction with Transformers\n",
        "\n",
        "Using the pre-trained model to extract hidden states as features:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 2000/2000 [01:45<00:00, 18.91 examples/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModel.from_pretrained(model_ckpt).to(device)\n",
        "\n",
        "def extract_hidden_states(batch):\n",
        "    inputs = {k: torch.tensor(v).to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
        "    with torch.no_grad():\n",
        "        last_hidden_state = model(**inputs).last_hidden_state\n",
        "    return {\"hidden_state\": last_hidden_state[:, 0].cpu().numpy()}\n",
        "\n",
        "emotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True, batch_size=8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training a Classifier\n",
        "\n",
        "Using the extracted features to train a logistic regression classifier:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.65      0.71      0.68       550\n",
            "         joy       0.71      0.80      0.75       704\n",
            "        love       0.49      0.30      0.37       178\n",
            "       anger       0.51      0.44      0.47       275\n",
            "        fear       0.55      0.56      0.55       212\n",
            "    surprise       0.54      0.27      0.36        81\n",
            "\n",
            "    accuracy                           0.63      2000\n",
            "   macro avg       0.57      0.51      0.53      2000\n",
            "weighted avg       0.62      0.63      0.62      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
        "X_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
        "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
        "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
        "\n",
        "lr_clf = LogisticRegression(max_iter=3000)\n",
        "lr_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr_clf.predict(X_valid)\n",
        "print(classification_report(y_valid, y_pred, target_names=emotions[\"train\"].features[\"label\"].names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Fine-tuning with Trainer API\n",
        "\n",
        "Training the full transformer model with Hugging Face Trainer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install accelerate>=0.26.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 21/500 03:22 < 1:25:01, 0.09 it/s, Epoch 0.08/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     14\u001b[39m training_args = TrainingArguments(\n\u001b[32m     15\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     num_train_epochs=\u001b[32m2\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     logging_dir=\u001b[33m\"\u001b[39m\u001b[33m./logs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m )\n\u001b[32m     25\u001b[39m trainer = Trainer(\n\u001b[32m     26\u001b[39m     model=model,\n\u001b[32m     27\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     tokenizer=tokenizer\n\u001b[32m     32\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:905\u001b[39m, in \u001b[36mDistilBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    897\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    898\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    899\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m    900\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m    901\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    903\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m distilbert_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    914\u001b[39m hidden_state = distilbert_output[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[32m    915\u001b[39m pooled_output = hidden_state[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:724\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33msdpa\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[32m    720\u001b[39m         attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[32m    721\u001b[39m             attention_mask, embeddings.dtype, tgt_len=input_shape[\u001b[32m1\u001b[39m]\n\u001b[32m    722\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:531\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    529\u001b[39m     all_hidden_states = all_hidden_states + (hidden_state,)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    538\u001b[39m hidden_state = layer_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:466\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[33;03mParameters:\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    463\u001b[39m \u001b[33;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m sa_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    475\u001b[39m     sa_output, sa_weights = sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnau\\miniconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:392\u001b[39m, in \u001b[36mDistilBertSdpaAttention.forward\u001b[39m\u001b[34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    389\u001b[39m k = shape(\u001b[38;5;28mself\u001b[39m.k_lin(key))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[32m    390\u001b[39m v = shape(\u001b[38;5;28mself\u001b[39m.v_lin(value))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m attn_output = unshape(attn_output)\n\u001b[32m    402\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.out_lin(attn_output)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "num_labels = 6\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=emotions_encoded[\"train\"],\n",
        "    eval_dataset=emotions_encoded[\"validation\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds_output = trainer.predict(emotions_encoded[\"test\"])\n",
        "print(f\"Test metrics: {preds_output.metrics}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
